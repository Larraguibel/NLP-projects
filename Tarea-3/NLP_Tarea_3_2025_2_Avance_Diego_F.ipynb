{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyoSWRNG4V6j"
      },
      "source": [
        "Pontificia Universidad Católica de Chile <br>\n",
        "Departamento de Ciencia de la Computación <br>\n",
        "IIC3670 -\tProcesamiento de Lenguaje Natural<br>\n",
        "Segundo Semestre 2025<br>\n",
        "\n",
        "\n",
        "<h1><center>Tarea 3: Generación de Texto </center></h1>\n",
        "        Profesor: Marcelo Mendoza<br>\n",
        "        Fecha de entrega: 17 de octubre\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqaM5wrU4V6o"
      },
      "source": [
        "## Indicaciones\n",
        "\n",
        "Deberás entregar **SOLO** el archivo .ipynb en el buzón respectivo en canvas.\n",
        "\n",
        "**IMPORTANTE**:\n",
        "- Se te dará puntaje tanto por código como por la manera en la que respondas las preguntas planteadas.\n",
        "- El notebook debe tener todas las celdas de código ejecutadas.\n",
        "- Cualquier instancia de copia resultará en un 1,1 como nota de curso.\n",
        "- Esta tarea requiere uso de GPU. Ir a \"Entorno de ejecución\" --> \"Cambiar tipo de entorno de ejecución\" --> Marcar \"GPU T4\".\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVRvJOvf3LzY"
      },
      "source": [
        "# Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xbjr9nT3OJo"
      },
      "source": [
        "\n",
        "\n",
        "En cuanto a la estructura de esta segunda tarea, se consideran 4 secciones diferentes.\n",
        "* Parte 1: Generación de texto con LLaMA 3.1\n",
        "* Parte 2: Fine-Tune con modelo T5\n",
        "* Parte 3: Preguntas teóricas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF99zSnD4V6o"
      },
      "source": [
        "# Librerías"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXQpTSi4AEuQ"
      },
      "source": [
        "### PARTE 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2zZrvpKZ99D",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvMWfN3pAEuR"
      },
      "source": [
        "### PARTE 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N2_RZTMZ99D",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WOcFvN44V6r"
      },
      "source": [
        "# Parte 1: Generación de Texto con modelo Base (15 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loIDVrR_AEuS"
      },
      "source": [
        "### 1.1 Investigue el modelo https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct. Hint: Qué tarea hace, cuantos parámetros tiene, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_54FrICAEuS"
      },
      "source": [
        "**RESPUESTA:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V9141QeAEuS"
      },
      "source": [
        "Es un modelo multilingual de 8 billones de parámetros (creado por Meta) y su objetivo es generar textos en múltiples idiomas. También existen otros modelos con más parámetros (70B y 405B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYP2YtymZ99E"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GdghvgMAEuS"
      },
      "source": [
        "### 1.2 Cargue el modelo previamente mencionado (entregue el token al modelo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYnPAZbLAEuS"
      },
      "source": [
        "**RESPUESTA:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8pXcV5XZ99E",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhV1-W8jAEuS"
      },
      "source": [
        "### 1.3 Haga un prompt para resumir texto y otro para traducir texto de español a inglés"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyEJ_evVAEuS"
      },
      "source": [
        "**RESPUESTA:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY_DxLc-Z99E",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qauo_ZKeAEuT"
      },
      "source": [
        "### 1.4 Investigue sobre las métricas ROUGE y BLEU. ¿Qué miden? ¿En qué casos es mejor usar una u otra? Compare ambas métricas. Además investigué otra métrica que podría ser útil para evaluar este tipo de modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVHdZ8CoAEuT"
      },
      "source": [
        "**RESPUESTA:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB71Npy8Z99E"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XRwsKj2Z99E",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMelnrhHAEuT"
      },
      "source": [
        "### 1.5 Utilice los 2 prompts sobre el siguiente texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKL29qVQX4UK"
      },
      "source": [
        "Isaac Asimov, un escritor de ciencia ficción, previó los peligros potenciales de los agentes autónomos de IA en 1942, mucho antes del desarrollo de los sistemas de IA. Creó las Tres Leyes de la Robótica como un medio para limitar esos riesgos. En el código de ética de Asimov, la primera ley prohíbe a los robots dañar activamente a los humanos o permitir que se les haga daño negándose a actuar. La segunda ley ordena a los robots obedecer a los humanos a menos que las órdenes no estén de acuerdo con la primera ley. La tercera ley ordena a los robots protegerse a sí mismos en la medida en que hacerlo esté de acuerdo con las dos primeras leyes.\n",
        "\n",
        "***Bibliografía:*** https://www.computerweekly.com/es/definicion/Que-es-la-etica-para-la-IA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu5RbzCnAEuT"
      },
      "source": [
        "**RESPUESTA:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOQuUkQlZ99E",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6tPCWT1-_lc"
      },
      "source": [
        "### 1.6 Aplicar las métricas sobre los 2 outputs según corresponda. Comente los resultados obtenidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwpCG1Xn__wu"
      },
      "source": [
        "Referecia RESUMEN (realizado por un modelo GPT): Isaac Asimov anticipó los riesgos de la IA en 1942 y formuló las Tres Leyes de la Robótica: los robots no deben dañar a los humanos, deben obedecer sus órdenes salvo que contradigan la primera ley, y deben protegerse a sí mismos siempre que no entren en conflicto con las dos primeras.\n",
        "\n",
        "Referencia TRADUCCIÓN (realizado por traductor de Google): Isaac Asimov, a science fiction writer, foresaw the potential dangers of autonomous AI agents in 1942, long before the development of AI systems. He created the Three Laws of Robotics as a means of limiting those risks. In Asimov's code of ethics, the first law prohibits robots from actively harming humans or allowing harm to come to humans by refusing to act. The second law commands robots to obey humans unless the orders are inconsistent with the first law. The third law commands robots to protect themselves as long as doing so is consistent with the first two laws."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNhnG-pZZ99E"
      },
      "source": [
        "**RESPUESTA:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpAlPpOEZ99E",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPfno9nQIz25"
      },
      "source": [
        "### 1.7 Comente los resultados anteriores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm0sO8_d2Xls"
      },
      "source": [
        "**RESPUESTA:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBL8jcXhI1JI"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y45vFKpZ0YYN"
      },
      "source": [
        "# Parte 2: Modelo T5 (30 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l031t-B5AEuV"
      },
      "source": [
        "### 2.1 Cargue tres modelos T5 (librería transformers) usando:\n",
        "* Pretrained Model/ t5-base\n",
        "* Encoder Model (T5 Encoder)\n",
        "* Conditional Generation Model (T5 For Conditional Generation)\n",
        "\n",
        "Para cada uno use un ejemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mw5hMqhAEuV"
      },
      "source": [
        "**RESPUESTA:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeE2YXB3Z99F",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npSgqJZ5AEuW"
      },
      "source": [
        "### 2.2 ¿Cuáles son las diferencias entre los tres modelos?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i5k7mfUAEuW"
      },
      "source": [
        "**RESPUESTA:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDk7_4Qm3-46"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvtYSzydAEuW"
      },
      "source": [
        "### 2.3 Utilice el dataset para entrenar sobre la tarea de traducción. El dataset a utilizar se encuentra en canvas. Debe evaluar con una de las métricas anteriores vistas en la sección 1.4.\n",
        "\n",
        "El dataset original se encuentra en el link: https://www.kaggle.com/datasets/dhruvildave/en-fr-translation-dataset. En canvas se encuentra un dataset reducido para poder hacer el entrenamiento. (HINT: recuerde separar la data en train y test y estudie los datos que les entrega el dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3OWehql35Tj"
      },
      "source": [
        "**RESPUESTA:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrSkfSsoZ99J",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGN_MPpteSG8"
      },
      "source": [
        "### 2.4 Realice una visualización sobre el train_loss con respecto a las epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esovicpveSy9"
      },
      "source": [
        "**RESPUESTA:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suJKD7W6Z99J",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRum3ny3JxQK"
      },
      "source": [
        "### 2.5 Comente sobre los resultados obtenidos en entrenamiento y con respecto a la métrica que usó para evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sdq6wvzYJ3Tk"
      },
      "source": [
        "**RESPUESTA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9DITxAuJ5X0"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iJex0tfZgiW"
      },
      "source": [
        "### 2.6 Cargue el modelo OPUS https://huggingface.co/Helsinki-NLP/opus-mt-tc-big-en-fr. Seleccionen una muestra del dataset (testing) de minimo tamaño 3 y realicen inferencias sobre este modelo, el modelo LLaMA y el modelo entrenado previamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USRQDkg9Z99J",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7wPHMs7KK2m"
      },
      "source": [
        "### 2.7 Haga una comparación entre los resultados de los tres modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uYLa8Jc37Y5"
      },
      "source": [
        "**RESPUESTA:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkiFSQgCg4kd"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0ql0cmrR79S"
      },
      "source": [
        "# Parte 3: Preguntas teóricas (15 puntos)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKVQYCaQ49NM"
      },
      "source": [
        "Para esta sección deberá ingresar a la siguiente página https://poloclub.github.io/transformer-explainer/ y responder las siguientes preguntas. **Es importante utilizar referencias**, basta con dejar el link en la pregunta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK4W82he3r2I"
      },
      "source": [
        "1. ¿Qué ocurre en la primera etapa del proceso del bloque del Transformer? (La parte inicial está a la izquierda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoP9Ifl1AEuX"
      },
      "source": [
        "**RESPUESTA:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to9MiiE6AEuX"
      },
      "source": [
        "La primera etapa es la de separar las palabras en tokens, este proceso es llamado tokenización, luego hay que asociar a cada token un vector, a estos vectores se les llama embeddings. Esta etapa es crucial porque los embeddings luego se entrenan para generar representaciones en espacios vectoriales, en estos espacios, por lo general, palabras similares están cerca en el sentido de distancia de vectores. Luego de esto a cada token se le asocia una posición, ya que en el lenguaje el orden de las palabras importa, a esta asociación se le llama positional encoding y es representado por vectores de la misma dimensión que el de los embeddings. Finalmente para unir todo se utiliza una suma de vectores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XVNvE6-AEuX"
      },
      "source": [
        "2. ¿Qué son y qué realizan Q, K y V en el transformer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHXeKlcH6VBf"
      },
      "source": [
        "**RESPUESTA:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcOTgt129TTv"
      },
      "source": [
        "Q, K y V son matrices. Q corresponde a la matriz de queries, K corresponde a la matriz de claves, V corresponde a la matriz de valores. Estas 3 matrices interactúan a través de operaciones matriciales y softmax para componer el mecanismo de atención. Q se compara con K para ver la relevancia y con esta se ve que tanto peso tendrá el value al momento de unir las representaciones. Los valores de estas matrices se ajustan por entrenamiento y permiten que el modelo vaya aprendiendo relaciones como gramática, relaciones lexicográgicas, etc. Estas matrices componen 1 head de atención, pero los modelos se componen de más de 1 head, por lo que cada head puede aprender distintas relaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaqbuJDyNff_"
      },
      "source": [
        "3. ¿Para qué sirve el Positional Encoding? ¿Qué tipos hay? Explique el funcionamiento de al menos 2 tipos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8lJ_wT6NsYr"
      },
      "source": [
        "**RESPUESTA:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61KxxRrPNttt"
      },
      "source": [
        "Información obtenida de: https://medium.com/thedeephub/positional-encoding-explained-a-deep-dive-into-transformer-pe-65cfe8cfe10b\n",
        "\n",
        "El positional encoding sirve para guardar información respecto a la posición del token directamente en su representación de embedding (Ya que como vimos antes, este encoding se suma al embedding del token). Esto es crucial en procesamiento de lenguaje ya que el orden de las palabras le da un significado distinto respecto a otro orden.\n",
        "\n",
        "Hay distintos tipos como por ejemplo: Absolute positional encoding, Rotary position embedding, learned positional encoding y relative positional encoding.\n",
        "\n",
        "## **Absolute positional encoding**:\n",
        "Cada posición tiene asignada una única representación. La más adoptada es la sinusoidal, utilizada en el famoso paper, \"Attention is all you need\". Este método asigna un vector a cada posición a partir de combinaciones de senos y cosenos de la siguiente manera:\n",
        "\n",
        "- Para cada posición en la secuencia, se genera un vector de números.\n",
        "\n",
        "- Cada número de este vector se calcula usando una función seno o coseno.\n",
        "\n",
        "- Se utilizan distintas frecuencias para las diferentes dimensiones del vector.\n",
        "\n",
        "Las codificaciones posicionales tienen la misma dimensión $d_{model}$ que los embeddings, de modo que ambas pueden sumarse. Aquí, $pos$ representa la posición y $i$ la dimensión. Es decir, cada dimensión de la codificación posicional corresponde a una función sinusoidal:\n",
        "\n",
        "$$\n",
        "PE_{(pos,i)} =\n",
        "\\begin{cases}\n",
        "\\sin\\left(\\dfrac{pos}{10000^{i/d_{model}}}\\right) & \\text{si } i \\text{ es par} \\\\\n",
        "\\cos\\left(\\dfrac{pos}{10000^{(i-1)/d_{model}}}\\right) & \\text{si } i \\text{ es impar}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "## **Rotary positional embeding**:\n",
        "En vez de crear un nuevo vector de positional encoding, este método rota el embedding del token. Está rotación conserva la norma de los embeddings y codifica la posición del token:\n",
        "\n",
        "- En lugar de sumar codificaciones posicionales separadas, RoPE rota los vectores de query y key en función de su posición en la secuencia.\n",
        "\n",
        "- La rotación preserva la magnitud (manteniendo la similitud entre tokens) mientras codifica la información posicional en el ángulo.\n",
        "\n",
        "Al calcular el producto punto entre los vectores rotados:\n",
        "\n",
        "- Las magnitudes siguen capturando la similitud entre tokens.\n",
        "\n",
        "- El ángulo relativo entre los vectores captura la proximidad posicional.\n",
        "\n",
        "Este enfoque permite que RoPE integre de forma fluida tanto la información del token como la información posicional en una sola operación, haciéndolo más eficiente y potencialmente más efectivo que los métodos tradicionales de codificación posicional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQFDwD6W9oHf"
      },
      "source": [
        "4. Explique el funcionamiento del multi-head desde la entrada de los datos hasta el output. (Puede y se recomienda usar de guía la página para la explicación)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9qfjgyR90u5"
      },
      "source": [
        "**RESPUESTA:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14s8q9zt9z24"
      },
      "source": [
        "\n",
        "## **Bloque Transformer**\n",
        "El bloque Transformer es la unidad principal de procesamiento del modelo.  \n",
        "Cada bloque tiene dos componentes:\n",
        "\n",
        "1. **Multi-head self-attention:** permite que los tokens compartan información entre sí.  \n",
        "2. **MLP (perceptrón multicapa):** refina los detalles individuales de cada token.  \n",
        "\n",
        "Los modelos apilan varios de estos bloques Transformer, de modo que las representaciones de los tokens se vuelven más ricas a medida que pasan por ellos.\n",
        "\n",
        "## **Self-Attention**\n",
        "El mecanismo de self-attention permite que el modelo determine qué partes de la entrada son más relevantes para cada token.  \n",
        "Así, logra capturar el significado y las relaciones, incluso entre palabras lejanas dentro de una secuencia.  \n",
        "\n",
        "En su versión multi-head, el modelo ejecuta varios procesos de atención en paralelo, cada uno centrado en distintos patrones o relaciones del texto.\n",
        "\n",
        "\n",
        "## **Query, Key y Value**\n",
        "Para realizar la autoatención, cada *embedding* de token se transforma en tres nuevos vectores:\n",
        "\n",
        "- **Query (Q)**  \n",
        "- **Key (K)**  \n",
        "- **Value (V)**  \n",
        "\n",
        "Estas transformaciones se obtienen aplicando diferentes pesos y sesgos al *embedding* de cada token.  \n",
        "Los parámetros (pesos y sesgos) se optimizan durante el entrenamiento.  \n",
        "\n",
        "Luego, las queries se comparan con las keys para medir la relevancia entre tokens, y esa relevancia se usa para ponderar las values, determinando cuánta “atención” recibe cada parte del texto.\n",
        "\n",
        "## **Multi-Head**\n",
        "Una vez creados los embeddings `Q`, `K` y `V`, el modelo los divide en varios heads o cabezas de atención.  \n",
        "Cada cabeza trabaja con un subconjunto más pequeño de estos vectores y se enfoca en distintos aspectos del texto: como la gramática, el significado o las relaciones a largo plazo.  \n",
        "\n",
        "Tener múltiples cabezas permite que el modelo aprenda muchos tipos de relaciones en paralelo, lo que enriquece su comprensión general.\n",
        "\n",
        "## **Masked Self-Attention**\n",
        "Dentro de cada cabeza, el modelo decide cuánto debe enfocarse cada token en los demás. Esto se realiza en tres pasos:\n",
        "\n",
        "1. **Producto punto:** se multiplican los vectores Query y Key correspondientes y se suman sus coincidencias para obtener los scores de atención.  \n",
        "2. **Máscara:** se ocultan los tokens futuros para que el modelo no \"mire hacia adelante\" (importante durante la generación de texto).  \n",
        "3. **Softmax:** los *scores* se convierten en probabilidades, asegurando que cada fila sume 1, concentrando la atención en los tokens más relevantes previos.\n",
        "\n",
        "\n",
        "## **Attention Output y Concatenación**\n",
        "Cada cabeza multiplica sus scores de atención por los vectores Value correspondientes, generando un output de atención,  \n",
        "que representa una versión refinada de cada token luego de incorporar el contexto.  \n",
        "\n",
        "Finalmente, las salidas de todas las cabezas se concatenan para formar un único vector del mismo tamaño que el embedding original.  \n",
        "Este vector pasa luego al MLP del bloque Transformer, que continúa refinando la representación del token.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgRhEDYPOUmc"
      },
      "source": [
        "5. Explique sobre las conexiones residuales, normalización de las capas y el dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U43dIUx7Oqp2"
      },
      "source": [
        "**RESPUESTA:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGjtjaerOsVc"
      },
      "source": [
        "## **Conexiones residuales**\n",
        "Las conexiones residuales son una característica adicional de los transformers que mejora su rendimiento. Este tipo de conexión consiste en sumar la entrada de una capa con su salida, lo que permite mantener la información y evitar que las señales de aprendizaje se debiliten al pasar por muchas capas. Gracias a esto, el modelo puede entrenar arquitecturas más profundas de forma estable y eficiente.\n",
        "\n",
        "\n",
        "## **Normalización de capas (Layer Normalization)**\n",
        "La normalización de capas ayuda a estabilizar tanto el entrenamiento como la inferencia, ajustando los valores de entrada para que su media y varianza se mantengan consistentes. Esto hace que el modelo sea menos sensible a los pesos iniciales y aprenda de manera más efectiva. En los transformers, esta normalización suele aplicarse antes del bloque de self-attention, antes del MLP, y en ocasiones antes de la salida final.\n",
        "\n",
        "\n",
        "## **Dropout**\n",
        "Durante el entrenamiento, el dropout apaga aleatoriamente algunas conexiones entre neuronas o valores numéricos para evitar que el modelo se ajuste en exceso (overfitting) a patrones específicos. De esta manera, el modelo aprende representaciones más generales y robustas. El dropout se utiliza únicamente durante el entrenamiento, ya que en la fase de inferencia se desactiva, lo que permite aprovechar todas las conexiones cuando el modelo hace predicciones.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSfc4xFM9VBz"
      },
      "source": [
        "6. ¿Qué es temperatura, top_k y top_p en los transformers? Explique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMhubA359g-C"
      },
      "source": [
        "**RESPUESTA:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FLDAu4Y9ixY"
      },
      "source": [
        "## **Temperatura**\n",
        "La temperatura es un parámetro que se aplica antes de convertir los *logits* (valores sin normalizar que produce el modelo) en probabilidades.  \n",
        "Controla el nivel de aleatoriedad del modelo al generar texto.  \n",
        "\n",
        "- Una temperatura baja (por ejemplo, 0.2) amplifica las diferencias entre los *logits*, haciendo que las opciones con mayor probabilidad sean aún más dominantes.  \n",
        "  Esto lleva a resultados más predecibles y deterministas.  \n",
        "- Una temperatura alta (por ejemplo, 1.0 o más) suaviza las diferencias entre los logits, permitiendo que tokens menos probables tengan más peso.  \n",
        "  Esto genera resultados más diversos y creativos.  \n",
        "\n",
        "En resumen:  \n",
        "- Temperatura baja: respuestas seguras y consistentes.  \n",
        "- Temperatura alta: respuestas variadas y creativas.  \n",
        "\n",
        "\n",
        "## **Estrategias de muestreo (Sampling Strategy)**\n",
        "\n",
        "Después de calcular las probabilidades, el modelo necesita una estrategia para elegir el siguiente token.  \n",
        "Existen distintos métodos de muestreo, entre los más comunes están:\n",
        "\n",
        "### **1. Greedy Search**\n",
        "Selecciona siempre el token con la mayor probabilidad.  \n",
        "Es un método rápido, pero puede producir texto repetitivo o poco natural.\n",
        "\n",
        "### **2. Top-k Sampling**\n",
        "En este método, solo se consideran los k tokens más probables (por ejemplo, los 50 más probables).  \n",
        "Luego, el modelo selecciona uno al azar dentro de ese conjunto reducido.  \n",
        "Esto limita las opciones a las más plausibles, manteniendo un cierto grado de creatividad controlada.\n",
        "\n",
        "### **3. Top-p Sampling (Nucleus Sampling)**\n",
        "En lugar de un número fijo de tokens, Top-p selecciona el conjunto más pequeño de tokens cuya probabilidad acumulada total sea al menos p (por ejemplo, 0.9).  \n",
        "De esta manera, se incluyen solo los tokens más relevantes hasta alcanzar el umbral de probabilidad definido, descartando los poco probables.  \n",
        "\n",
        "Finalmente, el modelo aplica Softmax sobre los logits restantes para convertirlos en probabilidades, y escoge un token al azar dentro del conjunto permitido.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWPlap6U0tAN"
      },
      "source": [
        "7. Coloque la siguiente frase: 'The NLP course explains' y genere las siguientes palabras con los siguientes **PARÁMETROS**: **TEMPERATURA** = 1 y **top_k** = 10. Genere un mínimo de 3 palabras y escribalos. Luego cambie la **TEMPERATURA** = 0 y **top_k** = 0 y genere un mínimo de 3 palabras. Finalmente coloque **TEMPERATURA** = 2 y **top_k** = 30. Escriba las palabras generadas para cada caso junto a su probabilidad y luego comente los resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRPm4z-q9Qhb"
      },
      "source": [
        "**RESPUESTA:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SAGt_IiAEuX"
      },
      "source": [
        "## **TEMPERATURA** = 1 y **top_k** = 10:\n",
        "The NLP course explains\n",
        "\n",
        "1. that, 0.1899\n",
        "2. the, 0.3688\n",
        "3. \", 0.116\n",
        "\n",
        "## **TEMPERATURA** = 0 y **top_k** = 0:\n",
        "(No se pudo con o temperatura ni 0 top_k, por lo que se utilizó 0.2 y 1 respectivamente)\n",
        "\n",
        "The NLP course explains\n",
        "\n",
        "1. the, 1\n",
        "2. basics, 1\n",
        "3. of, 1\n",
        "\n",
        "## **TEMPERATURA** = 2 y **top_k** = 30:\n",
        "\n",
        "The NLP course explains\n",
        "\n",
        "1. and, 0.0212\n",
        "2. analy, 0.0268\n",
        "3. zes, 0.9496\n",
        "\n",
        "Vemos que para el primer caso, las probabilidades que se escogieron estaban en un rango bajo-medio, debido al nivel de temperatura y la cantidad de palabras posibles a escoger (10). Para el segundo caso, con menor temperatura y con top_k 1, se ve que siempre se escoge una única palabra, la con mayor logits y se le asigna probabilidad de 1. Para el último caso se ve que se escogen palabras que tenían muy baja probabilidad, esto debido a la temperatura de 2 y que además se pueden escoger entren más palabras (30), aquí pasa algo que llama la atención ya que el último token se escoge con mucha probabilidad, esto es debido a que el token anterior y este, suelen estar juntos ya que componen una palabra con un significado, \"analyzes\". Por lo que aún intentado escoger tokens variados, se ve que igual los modelos están restringido a relaciones sub-léxicas, lo cuál hace mucho sentido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrTUt42Avg17"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
